{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'the', 'sentence', 'is', 'the', 'essential', 'soul', 'to', 'express', 'oneself', 'in', 'their', 'own', 'way', ',', 'then', 'the', 'paragraph', 'is', 'the', 'virtual', 'body', 'of', 'it', '.', 'The', 'Text', 'Generator', 'is', 'an', 'intelligent', 'tool', 'that', 'creates', 'random', 'text', 'incorporated', 'with', 'random', 'thoughts', '.', 'This', 'smart', 'tool', 'is', 'a', 'virtual', 'friend', 'of', 'yours', 'that', 'can', 'talk', 'to', 'you', 'in', 'multidimensional', 'thinking', '.', 'It', 'will', 'provide', 'you', 'with', 'thoughts', ',', 'concepts', ',', 'and', 'ideas', 'of', 'different', 'topics', 'that', 'will', 'not', 'only', 'assist', 'you', 'in', 'creating', 'new', 'knowledge', 'but', 'also', 'enhance', 'your', 'brain', 'function', '.']\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "#write a function for tokenization\n",
    "def tokenize(p):\n",
    "    return word_tokenize(p)\n",
    "p=\"\"\"If the sentence is the essential soul to express oneself in their own way, then\n",
    "the paragraph is the virtual body of it. The Text Generator is an intelligent tool\n",
    "that creates random text incorporated with random thoughts. This smart tool is a \n",
    "virtual friend of yours that can talk to you in multidimensional thinking. It will provide you \n",
    "with thoughts, concepts, and ideas of different topics that will not only assist you in creating \n",
    "new knowledge but also enhance your brain function.\"\"\"\n",
    "\n",
    "tokens= tokenize(p)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 4, 'is': 4, '.': 4, 'in': 3, ',': 3, 'of': 3, 'that': 3, 'you': 3, 'to': 2, 'virtual': 2, 'tool': 2, 'random': 2, 'with': 2, 'thoughts': 2, 'will': 2, 'If': 1, 'sentence': 1, 'essential': 1, 'soul': 1, 'express': 1, 'oneself': 1, 'their': 1, 'own': 1, 'way': 1, 'then': 1, 'paragraph': 1, 'body': 1, 'it': 1, 'The': 1, 'Text': 1, 'Generator': 1, 'an': 1, 'intelligent': 1, 'creates': 1, 'text': 1, 'incorporated': 1, 'This': 1, 'smart': 1, 'a': 1, 'friend': 1, 'yours': 1, 'can': 1, 'talk': 1, 'multidimensional': 1, 'thinking': 1, 'It': 1, 'provide': 1, 'concepts': 1, 'and': 1, 'ideas': 1, 'different': 1, 'topics': 1, 'not': 1, 'only': 1, 'assist': 1, 'creating': 1, 'new': 1, 'knowledge': 1, 'but': 1, 'also': 1, 'enhance': 1, 'your': 1, 'brain': 1, 'function': 1})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "def freq(tokens):\n",
    "    ans= Counter(tokens)\n",
    "    return ans\n",
    "print(Counter(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'Generator', 'If', 'It', 'Text', 'The', 'This', 'a', 'also', 'an', 'and', 'assist', 'body', 'brain', 'but', 'can', 'concepts', 'creates', 'creating', 'different', 'enhance', 'essential', 'express', 'friend', 'function', 'ideas', 'in', 'incorporated', 'intelligent', 'is', 'it', 'knowledge', 'multidimensional', 'new', 'not', 'of', 'oneself', 'only', 'own', 'paragraph', 'provide', 'random', 'sentence', 'smart', 'soul', 'talk', 'text', 'that', 'the', 'their', 'then', 'thinking', 'thoughts', 'to', 'tool', 'topics', 'virtual', 'way', 'will', 'with', 'you', 'your', 'yours']\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "extract=sorted(set(tokens))\n",
    "print(extract)\n",
    "print(len(extract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpyNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading numpy-1.25.2-cp310-cp310-win_amd64.whl (15.6 MB)\n",
      "     -------------------------------------- 15.6/15.6 MB 748.3 kB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.25.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_text for document:9\n",
      "['and', 'bus', 'for', 'joe', 'late', 'may', 'samantha', 'took', 'train', 'waited', 'was']\n",
      "\n",
      "Sentence:\n",
      "Joe waited for the train\n",
      "BoW Vector:\n",
      "[0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
      "\n",
      "Sentence:\n",
      "the train was late\n",
      "BoW Vector:\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1.]\n",
      "\n",
      "Sentence:\n",
      "May and Samantha took the bus\n",
      "BoW Vector:\n",
      "[1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import re\n",
    "\n",
    "def word_extraction(sentence):\n",
    "    ignore = [\"a\",\"the\",\"is\"]\n",
    "    words = re.sub(\"[^\\w]\",\" \",sentence).split()\n",
    "    cleaned_text = [w.lower() for w in words if w not in ignore]\n",
    "    return cleaned_text\n",
    "def tokenize(allsentences):\n",
    "    vocab = []\n",
    "    for sentence in allsentences:\n",
    "        w = word_extraction(sentence)\n",
    "        vocab.extend(w)\n",
    "    return sorted(list(set(vocab)))\n",
    "def generate_bow(allsentences):\n",
    "    vocab = tokenize(allsentences)\n",
    "    print(\"word_text for document:9\\n{0}\\n\".format(vocab))\n",
    "    for sentence in allsentences:\n",
    "        words = word_extraction(sentence)\n",
    "        bag_vector = numpy.zeros(len(vocab))\n",
    "        for w in words:\n",
    "            for i, word in enumerate(vocab):\n",
    "                if word == w:\n",
    "                    bag_vector[i] += 1\n",
    "        print(\"Sentence:\\n{0}\\nBoW Vector:\\n{1}\\n\".format(sentence,bag_vector))\n",
    "allsentences = [\"Joe waited for the train\", \"the train was late\", \"May and Samantha took the bus\"]\n",
    "generate_bow(allsentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to prepare your data through bag of words for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# how to prepare data (vector) through bow for regression \n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def tokenize(corpus):\n",
    "    words = []\n",
    "    for sentence in corpus:\n",
    "        w = word_extraction(sentence)\n",
    "        words.extend(w)\n",
    "        \n",
    "    words = sorted(list(set(words)))\n",
    "    return words\n",
    "\n",
    "def word_extraction(corpus):\n",
    "    ignore = [\"a\", \"the\", \"is\"]\n",
    "    words = re.sub(r'[^\\w]', ' ', corpus).split()\n",
    "    cleaned_text = [w.lower() for w in words if w.lower() not in ignore]\n",
    "    return cleaned_text\n",
    "\n",
    "def generate_bow(allSentences, vocab):\n",
    "    data = []\n",
    "    for corpus in allSentences:\n",
    "        words = word_extraction(corpus)\n",
    "        bag_vector = np.zeros(len(vocab))\n",
    "        for w in words:\n",
    "            for i, word in enumerate(vocab):\n",
    "                if word == w:\n",
    "                    bag_vector[i] += 1\n",
    "        data.append(bag_vector)\n",
    "    return np.array(data)\n",
    "\n",
    "# Example data\n",
    "allSentences = [\"Joe waited for the train\", \"the train was late\", \"May and Samantha took the bus\"]\n",
    "\n",
    "vocab = tokenize(allSentences)\n",
    "bow_data = generate_bow(allSentences, vocab)\n",
    "print(bow_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
